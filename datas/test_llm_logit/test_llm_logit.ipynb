{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26794aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 15:45:44,264 - modelscope - INFO - Got 7 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3499b1c2e995411685052a066b3dc31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 7 items:   0%|          | 0.00/7.00 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f290df9b74fc4e7598334b13d763087a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f547baee9cf4292ac8947a25df0c946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [merges.txt]:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af81377bb5da49989deadc131dd9fbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f9ef6d90184ebaae2f62d71888f5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/6.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caeacff7caa14286a07e5dc5ebd34e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee8c9b14d88408ab9df353508d35059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/7.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161578b3bc964d50948ad2d9cb820c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.json]:   0%|          | 0.00/2.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 15:45:45,359 - modelscope - INFO - Download model 'Qwen/Qwen2.5-0.5B-Instruct' successfully.\n",
      "2026-01-14 15:45:45,360 - modelscope - INFO - Creating symbolic link [/root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 15:45:46,553 - modelscope - INFO - Got 3 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf638c1fee0447a6858a130d01ecace3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 3 items:   0%|          | 0.00/3.00 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118cec6070514f7286f6cedd63c7c7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/942M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6e8fff44024f3895bfbb9898e7ee97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [LICENSE]:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19379825ebab46a6aa21408aa3e8975a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/4.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 15:46:43,129 - modelscope - INFO - Download model 'Qwen/Qwen2.5-0.5B-Instruct' successfully.\n",
      "2026-01-14 15:46:43,131 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 1. 选择并加载模型和 Tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 确保模型处于评估模式\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc8c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. 准备输入文本\n",
    "prompt = \"你是谁？\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\") # \"pt\" 代表 PyTorch tensors\n",
    "\n",
    "# 3. 执行单步推理（前向传播）\n",
    "# 我们不需要计算梯度，所以使用 no_grad() 来节省计算资源并避免意外的训练。\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    # 'outputs' 包含多个部分，我们最关心的是 'logits'\n",
    "    # logits 的形状通常是 [batch_size, sequence_length, vocab_size]\n",
    "    # 我们想要的是 *下一个* Token 的预测，所以我们取序列中 *最后一个* Token 的 logits\n",
    "    next_token_logits = outputs.logits[:, -1, :] # 形状变为 [batch_size, vocab_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b6c96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[14.7066, 14.6095, 14.1463, 14.1403, 13.7114, 13.2062, 13.0800, 12.9611,\n",
       "          12.7823, 12.7142]]),\n",
       " tensor([[ 35946,  56568, 104198, 105043,  97639,   9909,    220,  49434,  18493,\n",
       "          104786]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 4. 获取候选 Token 及其分数\n",
    "# 我们可以获取 Logit 值最高的 K 个 Token\n",
    "\n",
    "k = 10 # 我们想看 Top 10 的候选者\n",
    "\n",
    "# (a) 获取 Top-K Logits\n",
    "top_k_logits, top_k_indices = torch.topk(next_token_logits, k)\n",
    "top_k_logits, top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0d44519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.9804e-05, 8.3850e-04, 2.6763e-05,  ..., 8.1091e-10, 8.1152e-10,\n",
       "         8.1088e-10]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# (b) (可选) 将 Logits 转换为概率\n",
    "# 使用 Softmax 函数将 Logits 转换为概率分布\n",
    "probabilities = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848fa833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1209, 0.1097, 0.0690, 0.0686, 0.0447, 0.0270, 0.0238, 0.0211, 0.0176,\n",
       "          0.0165]]),\n",
       " tensor([[ 35946,  56568, 104198, 105043,  97639,   9909,    220,  49434,  18493,\n",
       "          104786]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_probs, top_k_indices_probs = torch.topk(probabilities, k) # 这里的 indices 应该和上面一样\n",
    "top_k_probs, top_k_indices_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33c9c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['æĪĳ',\n",
       " 'ä½ł',\n",
       " 'æĪĳæĺ¯',\n",
       " 'ä½łæĺ¯',\n",
       " 'æĪĳä»¬',\n",
       " 'ï¼Ī',\n",
       " 'Ġ',\n",
       " 'ĠæĪ',\n",
       " 'åľ¨',\n",
       " 'æĪĳåľ¨']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 5. 打印结果\n",
    "# 将 Token 索引解码回文本\n",
    "top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices[0]) # [0] 是因为 batch_size=1\n",
    "top_k_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e41bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['æĪĳ',\n",
       " 'ä½ł',\n",
       " 'æĪĳæĺ¯',\n",
       " 'ä½łæĺ¯',\n",
       " 'æĪĳä»¬',\n",
       " 'ï¼Ī',\n",
       " 'Ġ',\n",
       " 'ĠæĪ',\n",
       " 'åľ¨',\n",
       " 'æĪĳåľ¨']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_tokens_probs = tokenizer.convert_ids_to_tokens(top_k_indices_probs[0])\n",
    "top_k_tokens_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a956b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文本: '你是谁？'\n",
      "------------------------------\n",
      "模型预测的 Top-10 个下一个 Token (基于 Logits):\n",
      "  - Token: 'æĪĳ', Logit: 14.7066\n",
      "  - Token: 'ä½ł', Logit: 14.6095\n",
      "  - Token: 'æĪĳæĺ¯', Logit: 14.1463\n",
      "  - Token: 'ä½łæĺ¯', Logit: 14.1403\n",
      "  - Token: 'æĪĳä»¬', Logit: 13.7114\n",
      "  - Token: 'ï¼Ī', Logit: 13.2062\n",
      "  - Token: ' ', Logit: 13.0800\n",
      "  - Token: ' æĪ', Logit: 12.9611\n",
      "  - Token: 'åľ¨', Logit: 12.7823\n",
      "  - Token: 'æĪĳåľ¨', Logit: 12.7142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"输入文本: '{prompt}'\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"模型预测的 Top-{k} 个下一个 Token (基于 Logits):\")\n",
    "\n",
    "for token, logit in zip(top_k_tokens, top_k_logits[0]):\n",
    "    # 使用 .item() 从 Tensor 中提取 Python 数字\n",
    "    print(f\"  - Token: '{token.replace('Ġ', ' ')}', Logit: {logit.item():.4f}\") # 'Ġ' 通常代表空格\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def1160d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "模型预测的 Top-10 个下一个 Token (基于概率):\n",
      "  - Token: 'æĪĳ' -> 解码: '我', 概率: 12.0896%\n",
      "  - Token: 'ä½ł' -> 解码: '你', 概率: 10.9718%\n",
      "  - Token: 'æĪĳæĺ¯' -> 解码: '我是', 概率: 6.9035%\n",
      "  - Token: 'ä½łæĺ¯' -> 解码: '你是', 概率: 6.8624%\n",
      "  - Token: 'æĪĳä»¬' -> 解码: '我们', 概率: 4.4693%\n",
      "  - Token: 'ï¼Ī' -> 解码: '（', 概率: 2.6966%\n",
      "  - Token: 'Ġ' -> 解码: ' ', 概率: 2.3768%\n",
      "  - Token: 'ĠæĪ' -> 解码: ' �', 概率: 2.1105%\n",
      "  - Token: 'åľ¨' -> 解码: '在', 概率: 1.7649%\n",
      "  - Token: 'æĪĳåľ¨' -> 解码: '我在', 概率: 1.6486%\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 30)\n",
    "print(f\"模型预测的 Top-{k} 个下一个 Token (基于概率):\")\n",
    "\n",
    "for token, prob in zip(top_k_tokens_probs, top_k_probs[0]):\n",
    "    # 解码单个token为可读文本\n",
    "    decoded_token = tokenizer.decode([tokenizer.convert_tokens_to_ids(token)])\n",
    "    print(f\"  - Token: '{token}' -> 解码: '{decoded_token}', 概率: {prob.item():.4%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7797893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "如果选择 Logit 最高的 Token，下一个输入将是: '你是谁？我'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. (可选) 演示如何选择一个 Token 并继续生成\n",
    "chosen_token_id = top_k_indices[0][0].unsqueeze(0).unsqueeze(0) # 选择 Logit 最高的那个\n",
    "new_input_ids = torch.cat([input_ids, chosen_token_id], dim=-1)\n",
    "new_prompt = tokenizer.decode(new_input_ids[0])\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"如果选择 Logit 最高的 Token，下一个输入将是: '{new_prompt}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
